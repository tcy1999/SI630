{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import json\n",
    "import gzip\n",
    "\n",
    "random.seed(2021)\n",
    "np.random.seed(2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Only used for generating the data file for the first time\n",
    "def read_input(jsonname, txtname, line_limit=None):\n",
    "    with gzip.open(jsonname, 'r') as fin:        \n",
    "        jsonl_content = fin.read()        \n",
    "    \n",
    "    result = [json.loads(jline) for jline in jsonl_content.splitlines()]\n",
    "    if line_limit:\n",
    "        result = result[:line_limit]\n",
    "        # result = np.random.choice(result, line_limit)\n",
    "    \n",
    "    for item in result:\n",
    "        item['lyrics'] = item['lyrics'].replace('\\n', ' [LINE] ')[:1024]\n",
    "    \n",
    "    with open(txtname, 'w') as f:        \n",
    "        for item in result:\n",
    "            f.writelines(item['lyrics'] + '\\n')\n",
    "    return result\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the output of this notebook since I run the tasks separately, and the results seem messy\n",
    "import logging\n",
    "\n",
    "from simpletransformers.language_modeling import LanguageModelingModel, LanguageModelingArgs\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)\n",
    "\n",
    "train_file = \"train.txt\"\n",
    "dev_file = \"dev.txt\"\n",
    "test_file = \"test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using example code from Simple Transformers\n",
    "model_args = LanguageModelingArgs()\n",
    "model_args.reprocess_input_data = True\n",
    "model_args.overwrite_output_dir = True\n",
    "model_args.output_dir = \"./output1\"\n",
    "model_args.num_train_epochs = 1\n",
    "model_args.train_batch_size = 5\n",
    "model_args.learning_rate = 4e-7\n",
    "model_args.adam_epsilon = 1e-9\n",
    "model_args.dataset_type = \"simple\"\n",
    "model_args.mlm = False  # mlm must be False for CLM\n",
    "# model_args.evaluate_during_training = True\n",
    "# model_args.evaluate_during_training_verbose = True\n",
    "# model_args.evaluate_during_training_steps = 50000\n",
    "model_args.max_seq_length = 1024\n",
    "model_args.manual_seed = 2021  # Set for reproductivity\n",
    "\n",
    "model1 = LanguageModelingModel(\n",
    "    \"gpt2\", \"gpt2\", use_cuda=True, args=model_args\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model1.train_model(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "result = model1.eval_model(train_file)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model1.eval_model(dev_file)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = LanguageModelingArgs()\n",
    "model_args.reprocess_input_data = True\n",
    "model_args.overwrite_output_dir = True\n",
    "model_args.output_dir = \"./output2\"\n",
    "model_args.num_train_epochs = 1\n",
    "model_args.train_batch_size = 4\n",
    "model_args.learning_rate = 4e-6\n",
    "model_args.adam_epsilon = 4e-9\n",
    "model_args.dataset_type = \"simple\"\n",
    "model_args.mlm = False  # mlm must be False for CLM\n",
    "model_args.max_seq_length = 1024\n",
    "model_args.manual_seed = 2021\n",
    "\n",
    "model2 = LanguageModelingModel(\n",
    "    \"gpt2\", \"gpt2\", use_cuda=True, args=model_args\n",
    ")\n",
    "\n",
    "model2.train_model(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model2.eval_model(train_file)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model2.eval_model(dev_file)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = LanguageModelingArgs()\n",
    "model_args.reprocess_input_data = True\n",
    "model_args.overwrite_output_dir = True\n",
    "model_args.output_dir = \"./output3\"\n",
    "model_args.num_train_epochs = 3\n",
    "model_args.train_batch_size = 5\n",
    "model_args.learning_rate = 4e-7\n",
    "model_args.adam_epsilon = 1e-9\n",
    "model_args.dataset_type = \"simple\"\n",
    "model_args.mlm = False  # mlm must be False for CLM\n",
    "model_args.max_seq_length = 1024\n",
    "model_args.manual_seed = 2021\n",
    "\n",
    "model3 = LanguageModelingModel(\n",
    "    \"gpt2\", \"gpt2\", use_cuda=True, args=model_args\n",
    ")\n",
    "\n",
    "model3.train_model(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model3.eval_model(train_file)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model3.eval_model(dev_file)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model3.eval_model(test_file)  # model3 is the best one\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following line and comment the read file operation \n",
    "# when running for the first time to generate the corresponding input file\n",
    "# train = read_input('scratch/song-lyrics.train.jsonl.gz', 'train.txt', 100000)\n",
    "with open('train.txt', 'r') as f:        \n",
    "    train = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev = read_input('scratch/song-lyrics.dev.jsonl.gz', 'dev.txt')\n",
    "with open('dev.txt', 'r') as f:        \n",
    "    dev = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = read_input('scratch/song-lyrics.test.jsonl.gz', 'test.txt')\n",
    "with open('test.txt', 'r') as f:        \n",
    "    test = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# machine_train = read_input('scratch/lyrics.machine-gen.train.jsonl.gz', 'machine-train.txt')\n",
    "with open('machine-train.txt', 'r') as f:        \n",
    "    machine_train = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# machine_dev = read_input('scratch/lyrics.machine-gen.dev.jsonl.gz', 'machine-dev.txt')\n",
    "with open('machine-dev.txt', 'r') as f:        \n",
    "    machine_dev = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# machine_test = read_input('scratch/lyrics.machine-gen.test.jsonl.gz', 'machine-test.txt')\n",
    "with open('machine-test.txt', 'r') as f:        \n",
    "    machine_test = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_data(machine_data, song_data):\n",
    "    data = []\n",
    "    for item in song_data:\n",
    "        data.append([item, 1])\n",
    "    for item in machine_data:\n",
    "        data.append([item, 0])\n",
    "    df = pd.DataFrame(data)\n",
    "    df.columns = [\"text\", \"labels\"]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpletransformers.language_generation import LanguageGenerationModel, LanguageGenerationArgs\n",
    "\n",
    "model_args = LanguageGenerationArgs()\n",
    "model_args.manual_seed=2021\n",
    "model_args.max_length=1024\n",
    "text_generator = LanguageGenerationModel('gpt2', './output3', use_cuda=True,args=model_args)\n",
    "\n",
    "prompts = [\"My\", \"The\", \"One\", \"When\", \"If\"]\n",
    "\n",
    "for prompt in prompts:\n",
    "    generated = text_generator.generate(prompt)\n",
    "    generated = generated[0].replace('[LINE]', '\\n')\n",
    "    print('Prompt:', prompt, '\\n')\n",
    "    print('Generated text:', generated, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = np.random.choice(train, 500)\n",
    "generated = {}\n",
    "\n",
    "for index in range(0, len(sample)):\n",
    "    prompt = sample[index].split(' ',1)[0]\n",
    "    # print(prompt)\n",
    "    temp = text_generator.generate(prompt, args={'max_length': 1000})\n",
    "    generated[index] = temp[0].replace('[LINE]', '\\n')\n",
    "\n",
    "with open('generated.json', 'w') as file:\n",
    "    json.dump(generated, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('generated.json', 'r') as file:\n",
    "    generated = json.load(file)\n",
    "part2_data = []\n",
    "for index in range(0, 500):\n",
    "    text = generated[str(index)].replace('\\n', '[LINE]')\n",
    "    part2_data.append([text, 0])\n",
    "part2_df = pd.DataFrame(part2_data)\n",
    "part2_df.columns = [\"text\", \"labels\"]\n",
    "part2_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = full_data(machine_train, train)\n",
    "train_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df = full_data(machine_dev, dev)\n",
    "dev_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = full_data(machine_test, test)\n",
    "test_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "model_args = ClassificationArgs()\n",
    "model_args.num_train_epochs = 3\n",
    "model_args.train_batch_size = 5\n",
    "model_args.learning_rate = 4e-7\n",
    "model_args.adam_epsilon = 1e-8\n",
    "model_args.output_dir = \"./output-class\"\n",
    "model_args.manual_seed = 2021\n",
    "model_args.mlm = True\n",
    "\n",
    "model = ClassificationModel(\"distilbert\", \"distilbert-base-cased\", use_cuda=True, args=model_args)\n",
    "\n",
    "# Train the model\n",
    "model.train_model(train_df, acc=sklearn.metrics.accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result, model_outputs, wrong_predictions = model.eval_model(train_df, acc=sklearn.metrics.accuracy_score)\n",
    "# Probably due to the large size of data, the execution would sometimes stuck after evaluation. Restarting the kernel and \n",
    "# running the evaluations separately could solve the problem.\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result, model_outputs, wrong_predictions = model.eval_model(dev_df, acc=sklearn.metrics.accuracy_score)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result, model_outputs, wrong_predictions = model.eval_model(test_df, acc=sklearn.metrics.accuracy_score)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result, model_outputs, wrong_predictions = model.eval_model(part2_df, acc=sklearn.metrics.accuracy_score)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}